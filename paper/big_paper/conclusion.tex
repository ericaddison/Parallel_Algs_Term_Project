The Fast Fourier Transform has many real world applications. It is therefore crucial to implement it in an algorithm that is both efficient from a time and resource perspective. As shown above the $DFT$ algorithm is very easy to implement but runs in $T(n) = O(n^2)$ time. This is not tenable for large order transforms. From the above graphs it is clear that as the number of elements in input vectors scales a better solution is needed. We have discussed two algorithms that implement the FFT. The first is the recursive solution first (re-)discovered by Cooley-Tukey. With some manipulation and bit-twiddling the recursive algorithm can be transformed into an iterative one. This is significant since modern microprocessors generally perform iterative programs faster than recursive ones. 

This paper focused on two ways to parallelize the FFT algorithm. The first is by using the C/C++ extension cilk. Cilk exposes parallelism and uses the concept of work-stealing to the processor and benefits from significant performance improvement. The other way to parallelize the FFT was by implementing it on a GPU. Specifically, we used Cuda/Nvidia to implement a parallel version of the FFT. Overall, the Cuda algorithm bested any other algorithm by several orders of magnitude. However, since the GPU is run as a device attached to the main host CPU, it suffers from substantial time lag needed to send the input data and receive the output results. It is only when processing vectors of size greater than $2^{24}$ elements that Cuda's performance with this penalty breaks even with our Python-Numpy implementation. 

On the other hand, Cilk was a very good contender in terms of performance as the number of elements increased. However, from our results it is obvious that the overhead of parallelism is not worth it for smaller input vectors since performance lagged as compared to the traditional C++ algorithms. 